import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import numpy as np
import scipy.io as sio
import os
import time
import cv2
import json
from collections import OrderedDict
from guided_diffusion import utils
from guided_diffusion.create import create_model_and_diffusion_RS
from unet import UNet
from Dim_autoencoder import LR_decompose  # <-- Added this import
import matplotlib.pyplot as plt

# Pretrained VGG model (feature extractor)
class VGG16FeatureExtractor(nn.Module):
    def __init__(self):
        super(VGG16FeatureExtractor, self).__init__()
        vgg = models.vgg16(pretrained=True).features
        self.slice = vgg[:23]  # Use up to the 23rd layer (features before FC layers)
        self.eval()
        for param in self.parameters():
            param.requires_grad = False  # Don't update weights of VGG

    def forward(self, x):
        return self.slice(x)

# Perceptual loss function using VGG features
def perceptual_loss(pred, gt):
    # Normalize images between [0, 1] if necessary
    pred = (pred + 1) / 2
    gt = (gt + 1) / 2

    # Convert to 3-channel (RGB) if required (e.g., grayscale)
    if pred.size(1) == 1:  
        pred = pred.repeat(1, 3, 1, 1)
        gt = gt.repeat(1, 3, 1, 1)
    
    vgg = VGG16FeatureExtractor().cuda()  # Use the GPU if available
    pred_features = vgg(pred)
    gt_features = vgg(gt)

    # L2 loss on features
    loss = F.mse_loss(pred_features, gt_features)
    return loss

# Combined loss function with MSE and Perceptual Loss
def combined_loss(pred, gt, alpha=1.0, beta=0.1):
    mse_loss = F.mse_loss(pred, gt)
    perc_loss = perceptual_loss(pred, gt)
    return alpha * mse_loss + beta * perc_loss

# Load the test data
def LoadTest_256by256(path_test):
    scene_list = os.listdir(path_test)
    scene_list.sort()
    test_data = np.zeros((len(scene_list), 256, 256, 28))
    for i in range(len(scene_list)):
        scene_path = path_test + scene_list[i]
        img = sio.loadmat(scene_path)['img']
        test_data[i, :, :, :] = img[:256, :256, :28] 
    test_data = torch.from_numpy(np.transpose(test_data, (0, 3, 1, 2)))
    return test_data

# Adaptive SVD thresholding for unmixing
def Unmix_svd_3d(y, threshold=0.001):
    Rr = 3  # The dimensionality of the subspace
    
    # Reshape input tensor to shape (b, c, h*w)
    b, c, h, w = y.shape
    y = y.reshape(b, c, -1)
    
    # Perform SVD
    U, S, V = torch.svd(y)
    
    # Apply adaptive thresholding: discard small singular values
    S = torch.where(S > threshold, S, torch.tensor(0.0, device=S.device))
    
    # Reconstruct the matrix using the thresholded singular values
    E = U[:, :, :Rr].permute(0, 2, 1)  # Use only the top Rr components
    A = torch.matmul(E, S.diag_embed())  # Recompute A using the thresholded S

    # Reshape A back to the original spatial shape
    A = A.view(b, Rr, h, w)
    
    return A, E

# Diffusion function
def diffusion_3HSI(A_y, A_x, A_c, E_y, y):
    opt = {
        'baseconfig': 'base.json',
        'gpu_ids': "0",
        'dataroot': '',
        'batch_size': 1,
        'savedir': './results',
        'eta1': 1,
        'eta2': 2,
        'seed': 0,
        'dataname': '',
        'step': 100,
        'scale': 4,
        'kernelsize': 9,
        'sig': None,
        'samplenum': 1,
        'resume_state': 'I190000_E97_opt'
    }
    # Assuming 'base.json' contains the JSON-formatted data
    with open('./guided_diffusion/base.json', 'r') as json_file:
        json_str = json_file.read()
    opt = json.loads(json_str, object_pairs_hook=OrderedDict)
    opt = utils.dict_to_nonedict(opt)

    if opt.get('step'):
        opt['diffusion'] = {'diffusion_steps': opt['step']}

    device = torch.device("cuda")

    # Create model and diffusion process
    model, diffusion = create_model_and_diffusion_RS(opt)

    # Load model
    fix_diff = 1
    if fix_diff:
        gen_path = './guided_diffusion/I190000_E97_gen.pth'
        cks = torch.load(gen_path)
        new_cks = OrderedDict()
        for k, v in cks.items():
            newkey = k[11:] if k.startswith('denoise_fn.') else k
            new_cks[newkey] = v
        model.load_state_dict(new_cks, strict=False)
    model.to(device)
    model.eval()

    param = {'eta1': opt['eta1']}

    Ch, ms = A_y.shape[0], A_y.shape[-1]
    model_condition = {'A_x': A_x.to(device), 'A_c': A_c.to(device), 'A_y': A_y.to(device), 'E_y': E_y.to(device), 'y': y.to(device)}
    Rr = 3  # Spectral dimensionality of subspace

    sample = diffusion.p_sample_loop(model, (1, Ch, ms, ms),
                                     Rr=Rr,
                                     clip_denoised=True,
                                     model_condition=model_condition,
                                     param=param,
                                     save_root=None,
                                     progress=True,)

    sample = (sample + 1) / 2  # Must normalize
    return sample

# Unmixing function (combining with adaptive thresholding)
def Unmix(y, x): 
    # Load pretrained model for spectral unmixing
    Decompose_model = LR_decompose().cuda()
    pretrained_model_path = './exp/unmixing/model_epoch_17.pth'
    checkpoint = torch.load(pretrained_model_path)
    Decompose_model.load_state_dict({k.replace('module.', ''): v for k, v in checkpoint.items()}, strict=True)
    Decompose_model.eval()
    
    with torch.no_grad():
        X_y, X_x, A_y, A_x, E_y, E_x = Decompose_model(y, x)

    if E_x.shape[1] == 28:
        E_x = torch.unsqueeze(E_x, 1)
    if E_y.shape[1] == 28:
        E_y = torch.unsqueeze(E_y, 1)  

    return X_y, X_x, A_y, A_x, E_y, E_x

# Mixing function
def mix(A_hat, E_y, shape):
    bs, c, h, w = shape
    X_hat = torch.zeros(bs, c, h, w)
    for i in range(bs):
        A_hat_f_m = torch.reshape(A_hat[i, :, :, :], [3, 256*256])
        X_hat_m = torch.mm(torch.reshape(E_y[i, :, :], [28, 3]), A_hat_f_m)
        X_hat[i, :, :, :] = torch.reshape(X_hat_m, [28, 256, 256])
    return X_hat

# Test function
def test(Condi_net):
    test_path = "datasets/kaist_simu_data/"
    test_data = LoadTest_256by256(test_path)
    test_gt = test_data.cuda().float()
    noise = torch.load('noise_tensor_g03_1.pt')
    noise = noise.cuda()
    input_meas = test_gt + noise

    STU = 1
    if STU:
        X_y, X_x, A_y, A_x, E_y, E_x = Unmix(input_meas, test_gt)  # A_x from test_gt for visual reference or PSNR
    else:
        A_y, E_y = Unmix_svd_3d(input_meas)
        A_x, E_x = Unmix_svd_3d(test_gt)

    with torch.no_grad():
        A_c = Condi_net(A_y)

    start_time = time.time()
    y = input_meas
    z = torch.empty_like(A_y)
    diffIt_A = A_y.shape[0]
    for j in range(diffIt_A):
        z[j, :, :, :] = diffusion_3HSI(A_y[j, :, :, :], A_x[j, :, :, :], A_c[j, :, :, :], E_y[j, :, :], y[j, :, :, :])

    A_hat = z  # * A_init.max()
    e_time = time.time() - start_time
    print(f'Time {e_time}.')
    shape = test_gt.shape
    X_hat = mix(A_hat, E_y.detach(), shape)

    out_X = 1
    if out_X:
        pred = X_hat  # / X_hat.max()
        truth = test_gt
    else:
        pred = A_hat
        truth = A_x

    return pred, truth, input_meas, A_y.detach()

def calculate_psnr(gt, pred, max_val=255.0):
    """Calculate the PSNR (Peak Signal-to-Noise Ratio) between the ground truth and predicted images."""
    # Calculate MSE (Mean Squared Error)
    mse = torch.mean((gt - pred) ** 2)
    
    # If the MSE is 0, the images are identical, and the PSNR is infinite (max quality)
    if mse == 0:
        return 100  # Return a high value to indicate no error

    # Calculate PSNR using the formula: 
    # PSNR = 10 * log10((MAX_I^2) / MSE)
    psnr_value = 10 * torch.log10(max_val ** 2 / mse)
    return psnr_value

# Main function
def main():
    print('Testing model: Self-supervised HSI denoising via Diff-Unmix')
    unet = UNet(in_channels=3, out_channels=3).cuda()
    pretrained_model_path = './exp/condition_function/model/model_epoch_61.pth'

    checkpoint = torch.load(pretrained_model_path)
    unet.load_state_dict({k.replace('module.', ''): v for k, v in checkpoint.items()}, strict=True)
    unet.eval()
    pred, truth, input_meas, A_y = test(unet)

    # Show bands   
    show_image = 1
    if show_image:
        hh = 256
        ww = 256
        bb = 26  # Channel index
        pred = np.transpose(pred.detach().cpu().numpy(), (0, 2, 3, 1)).astype(np.float32)
        input_meas = np.transpose(input_meas.detach().cpu().numpy(), (0, 2, 3, 1)).astype(np.float32)
        truth = np.transpose(truth.cpu().numpy(), (0, 2, 3, 1)).astype(np.float32)

        channel_X = truth[0, :hh, :ww, bb]
        channel_Y = pred[0, :hh, :ww, bb]
        channel_Z = input_meas[0, :hh, :ww, bb]
        
        # Plot and show the images
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 5))

        ax1.imshow(channel_X, cmap='gray')
        ax1.set_title('GT')
        ax1.axis('off')

        ax2.imshow(channel_Y, cmap='gray')
        ax2.set_title('Diff-Unmix')
        ax2.axis('off')

        ax3.imshow(channel_Z, cmap='gray')
        ax3.set_title('Noisy')
        ax3.axis('off')

        plt.tight_layout()
        plt.show()

    # Calculate PSNR
    psnr_value = calculate_psnr(torch.tensor(truth), torch.tensor(pred))
    print(f'PSNR between GT and restored image: {psnr_value.item()} dB')

    # Save Images
    save_img = 1
    if save_img:
        OUTPUT_folder_rgb = './exp/images/'
        save_path_our = OUTPUT_folder_rgb + 'diff-unmix_s10_g03.png'
        output = np.squeeze((pred - pred.min()) / (pred.max() - pred.min()))[:, :, 14]
        cv2.imwrite(save_path_our, cv2.cvtColor(255 * output, cv2.COLOR_RGB2BGR))

        save_path_our = OUTPUT_folder_rgb + 'gt_s10.png'
        output_gt = np.squeeze((truth - truth.min()) / (truth.max() - truth.min()))[:, :, 20]
        cv2.imwrite(save_path_our, cv2.cvtColor(255 * output_gt, cv2.COLOR_RGB2BGR))

        # Save the noisy image as well
        save_path_our = OUTPUT_folder_rgb + 'noisy_s10.png'
        noisy_output = np.squeeze((input_meas - input_meas.min()) / (input_meas.max() - input_meas.min()))[:, :, 20]
        cv2.imwrite(save_path_our, cv2.cvtColor(255 * noisy_output, cv2.COLOR_RGB2BGR))

    save_mat = 0
    if save_mat:
        name = f'{opt.outf}Test_result_real_check.mat'
        print(f'Save reconstructed HSIs as {name}.')
        sio.savemat(name, {'truth': truth, 'pred': pred, 'noisy': input_meas})

if __name__ == '__main__':
    main()
